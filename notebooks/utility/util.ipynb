{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import col, lit, expr, to_utc_timestamp, to_timestamp\nfrom configparser import ConfigParser\nimport ast\nfrom delta.tables import DeltaTable\nfrom functools import reduce"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e3cabd4-97c8-4406-8d5d-81ed21e8a7f5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class DatabaseParameter:\n  def __init__(self, server, database, schema, table, user, password):\n    self.server = server\n    self.database = database\n    self.schema = schema\n    self.table = table\n    self.user = user\n    self.password = password\n  \n  def get_server(self):\n    return self.server\n    \n  def get_database(self):\n    return self.database\n    \n  def get_schema(self):\n    return self.schema\n    \n  def get_table(self):\n    return self.table.replace(\"$\",\"\")\n  \n  def get_sql_server_table(self):\n    return \"[\" + self.table + \"]\"\n      \n  def get_user(self):\n    return self.user\n    \n  def get_password(self):\n    return self.password"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2169a79f-d9bc-4555-8c34-835d921fa3c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class WatermarkRange:\n  def __init__(self, start_time, end_time):\n    self.start_time = start_time\n    self.end_time = end_time\n    \n  def get_start_time(self):\n    return self.start_time\n  \n  def get_end_time(self):\n    return self.end_time"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eba96d3e-d3e3-49e6-80af-4fca2ff8a0a3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class TableAttributes:\n  def __init__(self, tablename_to_timestamp_columns, \n               tablename_to_datetimeoffset_columns):\n    self.tablename_to_timestamp_columns = tablename_to_timestamp_columns\n    self.tablename_to_datetimeoffset_columns = tablename_to_datetimeoffset_columns\n    \n  def get_tablename_to_timestamp_columns(self):\n    return self.tablename_to_timestamp_columns\n  def get_tablename_to_datetimeoffset_columns(self):\n    return self.tablename_to_datetimeoffset_columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d75bb4e-6693-4f90-bc7b-f18872034cda"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["class GoldLayerFilter:\n  def __init__(self, table1, table2, table1_join_key, table2_join_key, filter_col, reporting_list):\n    self.table1 = table1\n    self.table2 = table2\n    self.table1_join_key = table1_join_key\n    self.table2_join_key = table2_join_key\n    self.filter_col = filter_col\n    self.reporting_list = reporting_list\n  \n  def get_table1(self):\n    return self.table1\n  \n  def get_table2(self):\n    return self.table2\n  \n  def get_table1_join_key(self):\n    return self.table1_join_key\n  \n  def get_table2_join_key(self):\n    return self.table2_join_key\n  \n  def get_filter_col(self):\n    return self.filter_col  \n  \n  def get_reporting_list(self):\n    return self.reporting_list"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21bafb20-ccae-450f-95ac-c451de68bb5b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def create_table_attributes(config_file):\n  try:\n    # {table_name>:<List of timestamp columns>}\n    tablename_to_timestamp_columns = read_config(config_file, \n                                            \"change_timezone_tables\", \n                                            \"timestamp_tables\")\n    \n    # {<table_name>:<List of timestamp columns>}\n    tablename_to_datetimeoffset_columns = read_config(config_file, \n                                                 \"change_timezone_tables\", \n                                                 \"datetimeoffset_tables\")\n    \n    return TableAttributes(tablename_to_timestamp_columns, \n                           tablename_to_datetimeoffset_columns)\n  except NoSectionError:\n    print(\"Section does not exist.\")\n    traceback.print_exc()\n    return"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f419955-f709-404f-acae-89b1e4029ee3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def set_spark_properties(partition):\n  '''\n  Set spark properties to avoid creation of meta parquet files.\n  '''\n  spark.conf.set(\"spark.sql.sources.commitProtocolClass\", \\\n                 \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\n  spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n  spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\",\"LEGACY\")\n  # spark optimization parameter to reduce default number of partitions\n  spark.conf.set(\"spark.sql.shuffle.partitions\", partition)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cacdeaf8-bac6-4f24-ab0f-746a1e54eebc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def file_reader(source, file_path, header=True, schema=True, multiline=True):\n  '''\n  Generic File Reader. Can read any text and binary files and returns the spark dataframe\n  '''\n  if source.lower() == \"parquet\":\n    return spark.read.format(\"parquet\").load(file_path)\n  elif source.lower() == \"csv\":\n    return spark.read.format(\"csv\").option(\"header\", header).option(\"inferSchema\", schema) \\\n                                   .load(file_path)\n  else:\n    raise Exception(\"Invalid source type. Please specify a valid source type.\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25b4a82f-2825-467c-a5f3-1d0a17a427f3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def file_writer(target, df, file_path, save_mode=\"overwrite\", part_cols=\"\"):\n  '''\n  Generic File Writer. Can write any text and binary files\n  '''\n  if len(part_cols) > 0:\n    df.write.format(target).mode(save_mode).partitionBy([col(x) for x in part_cols]) \\\n                           .save(file_path)\n  else:\n    df.repartition(Constant.PARTITION).write.format(target).mode(save_mode).save(file_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a423d4ee-2e47-4dc9-ab32-c5d58796d0d2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def table_reader_delta(db_name, table_name, columns=\"\"):\n  '''\n  Delta Table Reader. Read the delta table and returns the spark dataframe\n  '''\n  if len(columns) > 0:\n    return spark.table(db_name + \".\" + table_name).select([col(x) for x in columns])\n  else:\n    return spark.table(db_name + \".\" + table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b348127a-5ce8-4fb6-9867-50886fbabd4d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def table_reader_jdbc(server, db_name, table_name, user, passwd, reader_type, query=None):\n  '''\n  Generic JDBC Reader. Can read from any JDBC source and returns the spark dataframe\n  '''\n  if reader_type == \"table\":\n    return spark.read.format(\"jdbc\") \\\n                     .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n                     .option(\"url\", server + db_name) \\\n                     .option(\"user\", user) \\\n                     .option(\"password\", passwd) \\\n                     .option(\"dbtable\", table_name) \\\n                     .load()\n  elif reader_type == \"query\":\n    return spark.read.format(\"jdbc\") \\\n                     .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n                     .option(\"url\", server + db_name) \\\n                     .option(\"user\", user) \\\n                     .option(\"password\", passwd) \\\n                     .option(\"query\", query) \\\n                     .load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37cc2dd4-defb-4af5-aa6c-2948228ad9e2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def table_writer_jdbc(df, server, db_name, table_name, user, passwd):\n  '''\n  Generic JDBC Writer. Can write to any JDBC supported database\n  '''\n  return df.write.format(\"jdbc\") \\\n                 .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n                 .option(\"url\", server + db_name) \\\n                 .option(\"user\", user) \\\n                 .option(\"password\", passwd) \\\n                 .option(\"dbtable\", table_name) \\\n                 .mode(\"overwrite\") \\\n                 .save()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c02063a6-7a4b-4df2-91b8-cbc9d9c3cfab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def table_writer_delta(df, db_name, table_name, save_mode, table_type=\"managed\", file_path=None):\n  '''\n  Delta Table Writer. Writes to specified Delta table\n  '''\n  if table_type.lower() == \"external\":\n    df.write.format(\"delta\").mode(save_mode).option(\"overwriteSchema\", \"true\") \\\n                            .option(\"path\", file_path).saveAsTable(db_name + \".\" + table_name)\n  elif table_type.lower() == \"managed\":\n    df.write.format(\"delta\").mode(save_mode).option(\"overwriteSchema\", \"true\") \\\n                            .saveAsTable(db_name + \".\" + table_name)\n  spark.catalog.refreshTable(db_name + \".\" + table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b113b65-183a-4fa0-bb49-b0ea6bcacca9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_config(config_file, section, option):\n  '''\n  Read the python configuration file and returns the option as list\n  '''\n  with open(config_file, \"r\") as file:\n    parser = ConfigParser() \n    parser.read(config_file)\n    \n  return ast.literal_eval(parser.get(section, option))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d93f0319-04e0-42fe-a7b7-9612b2514f4e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_delta_table(table_path):\n  '''\n  Delta Table Reader using Python API. Returns the DeltaTable object for a specified table\n  '''\n  return DeltaTable.forPath(spark, table_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b8845b6-20bf-4a4e-a839-44444b3297b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def update_watermark(table_path, table, new_watermark_value):\n  '''\n  Update the watermark table in Delta lake\n  '''\n  try:\n    deltaTable = get_delta_table(table_path)\n    deltaTable.update(\n      condition = col(\"table_name\") == table,\n      set = { \"watermark_value\": lit(new_watermark_value) } )\n  except Exception:\n    traceback.print_exc()\n    raise Exception(\"Error occured when updating watermark for \" + table)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d053640e-dac6-43fa-b774-4582994e1b2b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_database_parameters(server, table, table_namespace_parts, user, password):\n  '''\n  returns the object of DatabaseParameters \n  '''\n  # valid table format - <db_name>.<schema_name>.<table_name>\n  table_parts = table.split(\".\")\n  if len(table_parts) != table_namespace_parts:\n    raise Exception(\"Config file is not in correct format \" + table)\n        \n  return DatabaseParameter(server, \\\n                            table_parts[0].strip(), \\\n                            table_parts[1].strip(), \\\n                            table_parts[2].strip(), \\\n                            user, \\\n                            password)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e76a41dd-b2cf-4a27-91a1-2713a36d5e92"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def create_file_path(container_name, db_params, pipeline_runtime):\n  '''\n  If query end_time is 2020-09-01 14:20:55, then files are copied to\n  /mnt/bronze/<db_name>/<table_name>/2020_09_01/2020_09_01-14_20_55/\n  '''\n  date_time_folder = pipeline_runtime.replace(\"-\", \"_\").replace(\":\", \"_\") \\\n                                             .replace(\" \",\"-\")\n  date_folder = date_time_folder.split(\"-\")[0]\n  return container_name + \"/\" + db_params.get_database() + \"/\" + db_params.get_table() + \"/\" \\\n                              + date_folder + \"/\" + date_time_folder"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"760e3993-6ef4-45af-a623-91809315aab9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def columns_to_unix_timestamp(df, table_name, table_attributes):\n  df = datetime_columns_to_unix_timestamp(df, table_name, table_attributes)\n  df = datetimeoffset_columns_to_unix_timestamp(df, table_name, table_attributes)\n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da77a513-20fc-4fc5-9123-7451fe0921fe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def datetime_columns_to_unix_timestamp(df, table_name, table_attributes):\n  '''\n  This method converts datetime columns to unix timestamps based on the given table attributes\n  '''\n  column_list = table_attributes.get_tablename_to_timestamp_columns().get(table_name)\n  if column_list:\n    df = reduce(sql_datetime_to_unix_timestamp, column_list,  df)\n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ef8ef9f-9502-474c-abca-50570c5b0ba3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def datetimeoffset_columns_to_unix_timestamp(df, table_name, table_attributes):\n  '''\n  This method converts datetimeoffset columns to unix timestamps based on the given table attributes\n  '''\n  column_list = table_attributes.get_tablename_to_datetimeoffset_columns().get(table_name)\n  if column_list:\n    df = reduce(sql_datetimeoffset_to_unix_timestamp, column_list,  df)\n  return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b601b497-0c14-4b67-bf5b-59de55f06df4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def sql_datetime_to_unix_timestamp(source_df, column_zoneid_tuple):\n  '''\n  Convert datetimes with the supplied timezone to unix timestamps\n  column_zoneid_tuple - e.g. (\"EnterDate\", \"EST\")\n  '''\n  return source_df.withColumn(column_zoneid_tuple[0], to_utc_timestamp(column_zoneid_tuple[0], \n                                                                             column_zoneid_tuple[1]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af0936d0-d8e6-4a30-a03b-37556f6fb41c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def sql_datetimeoffset_to_unix_timestamp(source_df, column_format_tuple):\n  '''\n  Convert datetimeoffset to UTC if you pass in a zone-offset\n  column_format_tuple - e.g. (\"ModifiedDate\", \"yyyy-MM-dd HH:mm:ss z\")\n  '''\n  return source_df.withColumn(column_format_tuple[0], to_timestamp(column_format_tuple[0], \n                                                                       column_format_tuple[1]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eee36549-0648-4bfb-9939-4c718108bb21"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_stripped_param(widget_name):\n  return dbutils.widgets.get(widget_name).strip()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"feb0cb05-907a-4077-a358-1ec7c32edb9f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def get_primary_keys(config_file, section):\n  '''\n  Returns a list of primary keys\n  '''\n  try:\n    primary_keys = read_config(config_file, section, \"primary_keys\")\n  except NoSectionError:\n    traceback.print_exc()\n    raise Exception(\"Section primary_keys does not exist in config file \" + config_file)\n  return primary_keys"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce6be8e4-7521-4cf6-b258-772fe5fe88b4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"util","dashboards":[],"language":"python","widgets":{},"notebookOrigID":4043258465741102}},"nbformat":4,"nbformat_minor":0}
